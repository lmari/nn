{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain: il più semplice esempio di Q&A sul contenuto di un documento\n",
    "Luca Mari, maggio 2023  \n",
    "[virtenv `langchain`: langchain, openai]\n",
    "\n",
    "Il problema che si vuole risolvere qui è di usare un LLM per fare domande in linguaggio naturale a proposito del contenuto di uno o più documenti che non sono stati letti durante il pre-training.\n",
    "\n",
    "Se il documento è sufficientemente breve, tale da essere interamente contenuto in un prompt insieme con la domanda, il processo è molto semplice. Si può infatti passare direttamente il documento all'oggetto `load_qa_chain` come contesto, dunque senza doverlo dividere in chunks e senza dover fare l'embedding dei chunks (d'altra parte, se il documento non è abbastanza breve, l'esecuzione genera un errore, proprio dovuto all'eccessiva lunghezza del prompt inviato all'LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senza il contesto: The fourth element of the 8-tuple is the \"exception\" element, and its purpose is to provide information about any errors that may have occurred during the execution of the program. This element is typically used to store an exception object that contains information about the error, such as the type of error, the line of code where the error occurred, and a description of the error.\n",
      "Con il contesto: The fourth element of the 8-tuple is the set Ω ⊆ {u: T → U} of the admissible input functions of the model. Its purpose is to specify constraints on the functions that the model accepts as inputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# leggi il documento, breve\n",
    "loader = TextLoader('./8tuple_s.txt', encoding='utf8')\n",
    "data = loader.load()\n",
    "\n",
    "# crea la domanda\n",
    "query = \"What is the fourth element of the 8-tuple and what is its purpose?\"\n",
    "\n",
    "# attiva l'llm\n",
    "llm = OpenAI(temperature=0, client='simpleQA')\n",
    "\n",
    "# invia la domanda senza il contesto, ottieni e visualizza la risposta\n",
    "print('Senza il contesto:', llm(query).strip())\n",
    "\n",
    "# invia la domanda con il contesto, ottieni e visualizza la risposta\n",
    "chain = load_qa_chain(llm, chain_type='stuff')\n",
    "print('Con il contesto:', chain.run(input_documents=data, question=query).strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si vede, senza il contesto, cioè senza la lettura del documento, la risposta è sbagliata, mentre con il contesto, cioè con la lettura del documento, il risultato è di buona qualità.  \n",
    "È dunque solo se il documento è sufficientemente lungo che occorre dividerlo in chunks, farne l'embedding, e attivare la ricerca semantica per selezionare i chunks rilevanti per la specifica domanda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
